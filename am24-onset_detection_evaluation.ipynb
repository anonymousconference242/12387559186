{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate (Aubio) Methods for Onset Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.aGPTset.ExpressiveGuitarTechniquesDataset as agptset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "import librosa\n",
    "import aubio\n",
    "import mir_eval\n",
    "import random\n",
    "\n",
    "import am24utils\n",
    "\n",
    "dataset = agptset.import_db()\n",
    "\n",
    "DOTEST = False\n",
    "VERBOSE = False\n",
    "DB_PATH = 'dataset/aGPTset'\n",
    "SR = 48000\n",
    "printVerbose = lambda x: print(x) if VERBOSE else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering the Dataset...\")\n",
    "filtered_notes_db, filtered_files_db = am24utils.filter_files_db(dataset)\n",
    "#make sure that no audio_file_path contains \"impro\"\n",
    "assert filtered_notes_db.index.get_level_values(1).str.contains('impro').sum() == 0, \"Some audio_file_path contain 'impro' (%i)\"%(filtered_notes_db.index.get_level_values(1).str.contains('impro').sum())\n",
    "print(\"Done (%i notes in the filtered db).\"%len(filtered_notes_db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onsetlist_filename(filtered_notes_db:pd.DataFrame, filtered_files_db:pd.DataFrame):\n",
    "    onsetlist_sample = []\n",
    "    onsetlist_time = []\n",
    "    filenames = []\n",
    "    players = []\n",
    "    for file in filtered_notes_db.index.get_level_values(1).unique():\n",
    "        if file in filtered_files_db.index:\n",
    "            afp = filtered_files_db[filtered_files_db.index == file].full_audiofile_path.values\n",
    "            assert len(afp) == 1, \"More than one audio file path for file %s\"%file\n",
    "            filenames.append(afp[0])\n",
    "            cur_onset_list = filtered_notes_db.loc[filtered_notes_db.index.get_level_values(1) == file].onset_label_samples.values\n",
    "            cur_onset_list = [int(x) for x in cur_onset_list]\n",
    "            onsetlist_sample.append(cur_onset_list)\n",
    "            onsetlist_time.append(librosa.samples_to_time(cur_onset_list, sr=SR))\n",
    "            cur_player = filtered_files_db[filtered_files_db.index == file].player_id.values\n",
    "            assert len(cur_player) == 1, \"More than one player for file %s\"%file\n",
    "            cur_player = int(cur_player[0])\n",
    "            players.append(cur_player)\n",
    "        else:\n",
    "            raise ValueError(\"File %s not found in the files db\"%file)\n",
    "        \n",
    "    return onsetlist_sample, onsetlist_time, filenames, players\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "sample_onsetlist, time_onsetlist, filenames, playerlist  = get_onsetlist_filename(filtered_notes_db,filtered_files_db)\n",
    "assert len(sample_onsetlist) == len(time_onsetlist) == len(filenames) == len(playerlist), \"Different lengths for onsetlist, filenames and playerlist\"\n",
    "packedData = (time_onsetlist,filenames,playerlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Check:\n",
    "\n",
    "Usando **aubio** ho ottenuto il seguente errore con alcuni file\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[60], line 4\n",
    "      2 for method in AUBIO_ONSET_METHODS:\n",
    "      3     print(\"Computing onsets using %s method...\"%method)\n",
    "----> 4     detected_onsets[method] = compute_onsets(packedData, method=method)\n",
    "      5     print(\"Done.\")\n",
    "\n",
    "Cell In[59], line 6\n",
    "      4 detected_onsets = []\n",
    "      5 for i, filename in enumerate(filenames):\n",
    "----> 6     onsets = detect_onsets(os.path.join(DB_PATH,filename), method=method, sr=SR)\n",
    "      7     detected_onsets.append(onsets)\n",
    "      8     if DOTEST:\n",
    "\n",
    "Cell In[56], line 9\n",
    "      7 hop_size = 256\n",
    "      8 win_size = 512\n",
    "----> 9 s = aubio.source(filename, samplerate, hop_size)\n",
    "     11 # Select the onset detection method\n",
    "     12 if method in AUBIO_ONSET_METHODS:\n",
    "\n",
    "RuntimeError: AUBIO ERROR: source_wavread: Failed opening dataset/aGPTset/data/audio/acoustic_guitar_percussive_thumb_mf_DomSte_20200922_additional-500.wav (not encoded with PCM)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all files are PCM\n",
    "import wave\n",
    "import soundfile as sf\n",
    "\n",
    "def wave_is_pcm_encoded(filename):\n",
    "    try:\n",
    "        with wave.open(filename, 'rb') as wf:\n",
    "            # Check if the file has PCM encoding\n",
    "            return wf.getcomptype() == 'NONE' and wf.getsampwidth() != 0\n",
    "    except wave.Error:\n",
    "        # Failed to open the file or it's not a WAV file\n",
    "        return False\n",
    "    \n",
    "    \n",
    "############################################################################\n",
    "# Check if the file is PCM encoded using wave\n",
    "print(20*\"=\",\"Checking if the files are PCM encoded using wave...\")\n",
    "pcm_files = 0\n",
    "non_pcm_files = 0\n",
    "list_non_pcm_files = []\n",
    "for filename in filenames:\n",
    "    if wave_is_pcm_encoded(os.path.join(DB_PATH,filename)):\n",
    "        pcm_files += 1\n",
    "    else:\n",
    "        non_pcm_files += 1\n",
    "        list_non_pcm_files.append(filename)\n",
    "        \n",
    "print(\"Total file analyzed\", pcm_files+non_pcm_files)\n",
    "# Percetange of non-PCM files and PCM files\n",
    "print(\"Percentage of PCM files: %f\"%(pcm_files/(pcm_files+non_pcm_files)*100))\n",
    "print(\"Percentage of non-PCM files: %f\"%(non_pcm_files/(pcm_files+non_pcm_files)*100))\n",
    "\n",
    "# Print the non-PCM files\n",
    "print(f\"Non PCM files: {len(list_non_pcm_files)}\")\n",
    "for file in list_non_pcm_files:\n",
    "    printVerbose(file)\n",
    "\n",
    "print(20*\"=\",\"Done.\")\n",
    "############################################################################\n",
    "    \n",
    "\n",
    "def sf_is_pcm_encoded(filename):\n",
    "    try:\n",
    "        # Read the audio data\n",
    "        audio_data, _ = sf.read(filename)\n",
    "\n",
    "        # Check the data type of the audio data\n",
    "        data_type = audio_data.dtype\n",
    "\n",
    "        # PCM files typically have data types like 'int16' or 'int32'\n",
    "        if 'int' in str(data_type):\n",
    "            #print(f\"{filename} is encoded with PCM: {data_type}\")\n",
    "            return True\n",
    "        else:\n",
    "            #print(f\"{filename} is not encoded with PCM: {data_type}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to check encoding for {filename}: {e}\")\n",
    "    \n",
    "    \n",
    "# Check if the file is PCM encoded using soundfile\n",
    "print(20*\"=\",\"Checking if the files are PCM encoded using soundfile...\")\n",
    "pcm_files = 0\n",
    "non_pcm_files = 0\n",
    "list_non_pcm_files = []\n",
    "for filename in filenames:\n",
    "    if sf_is_pcm_encoded(os.path.join(DB_PATH,filename)):\n",
    "        pcm_files += 1\n",
    "    else:\n",
    "        non_pcm_files += 1\n",
    "        list_non_pcm_files.append(filename)\n",
    "        \n",
    "print(\"Total file analyzed\", pcm_files+non_pcm_files)\n",
    "# Percetange of non-PCM files and PCM files\n",
    "print(\"Percentage of PCM files: %f\"%(pcm_files/(pcm_files+non_pcm_files)*100))\n",
    "print(\"Percentage of non-PCM files: %f\"%(non_pcm_files/(pcm_files+non_pcm_files)*100))\n",
    "print(20*\"=\",\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/audio/acoustic_guitar_pitched_allstring2_naturalharmonics_f_LucTur_20200806.wav\"\n",
    "idx = filenames.index(filename)\n",
    "sample_onset = sample_onsetlist[idx]\n",
    "time_onset = time_onsetlist[idx]\n",
    "print(\"ground truth onsets for file %s:\"%filename)\n",
    "print(time_onset)\n",
    "print(sample_onset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onset Detection (AUBIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUBIO_ONSET_METHODS = ['hfc', 'energy', 'complex', 'phase', 'specdiff', 'kl', 'mkl', 'specflux']\n",
    "HOP_SIZE = 64\n",
    "WIN_SIZE = 64\n",
    "\n",
    "def detect_onsets(filename, method='default', samplerate=SR, hop_size=HOP_SIZE, win_size=WIN_SIZE):\n",
    "    # Open the audio file\n",
    "    s = aubio.source(filename, samplerate, hop_size)\n",
    "\n",
    "    # Select the onset detection method\n",
    "    if method in AUBIO_ONSET_METHODS:\n",
    "        onset_detector = aubio.onset(method, win_size, hop_size, samplerate)\n",
    "        # change the threshold\n",
    "        onset_detector.set_threshold(0.9) \n",
    "        onset_detector.set_silence(-51.7) \n",
    "    else:\n",
    "        print(f\"Invalid onset detection method: {method}\")\n",
    "        return None\n",
    "\n",
    "    onsets = []\n",
    "    while True:\n",
    "        samples, read = s()\n",
    "        if onset_detector(samples):\n",
    "            # Get the timestamp of the detected onset\n",
    "            timestamp = onset_detector.get_last_s()\n",
    "            onsets.append(timestamp)\n",
    "        if read < hop_size:  # end of file reached\n",
    "            break\n",
    "\n",
    "    return onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCM file\n",
    "print(\"Trying to detect onsets in PCM file...\")\n",
    "filename = \"pcm_file_test.wav\"\n",
    "try:\n",
    "    onsets = detect_onsets(filename, method='hfc', samplerate=SR)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    onsets = None\n",
    "print(\"Done...\")\n",
    "\n",
    "# NON-PCM file\n",
    "print(\"Trying to detect onsets in NON-PCM file...\")\n",
    "filename = \"non_pcm_file_test.wav\"\n",
    "try:   \n",
    "    onsets = detect_onsets(filename, method='hfc', samplerate=SR)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    onsets = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveform, groundtruth onsets and detected onsets\n",
    "def plot_onsets(filename, groundtruth_onset, detected_onsets, method):\n",
    "    y, sr = librosa.load(filename, sr=None)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.waveshow(y=y, sr=sr, color='blue')\n",
    "    plt.vlines(groundtruth_onset, -0.5, 0.5, color='g', linestyle='--', label='Ground truth onsets')\n",
    "    # Plot detected onsets as crosses \n",
    "    plt.scatter(detected_onsets, np.zeros_like(detected_onsets), color='r', marker='x', label='Detected onsets')\n",
    "    plt.legend()\n",
    "    plt.title(\"Onset detection using %s method\" % method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of plotting the waveform, groundtruth onsets and detected onsets\n",
    "while True:\n",
    "    filename = random.choice(filenames)\n",
    "    if wave_is_pcm_encoded(os.path.join(DB_PATH,filename)):\n",
    "        break\n",
    "groundtruth_onset = time_onsetlist[filenames.index(filename)]\n",
    "for method in AUBIO_ONSET_METHODS:\n",
    "    detected_onsets = detect_onsets(os.path.join(DB_PATH, filename), method=method, samplerate=48000)\n",
    "    plot_onsets(os.path.join(DB_PATH, filename), groundtruth_onset, detected_onsets, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute onset times using different aubio methods\n",
    "def compute_onset_list(packedData, method='default'):\n",
    "    print(\"Computing onsets using %s method...\"%method)\n",
    "    time_onsetlist, filenames, playerlist = packedData\n",
    "    detected_onsets = []\n",
    "    ground_truth_onsets = []\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if wave_is_pcm_encoded(os.path.join(DB_PATH,filename)):\n",
    "            onsets = detect_onsets(os.path.join(DB_PATH,filename), method=method, samplerate=SR)\n",
    "            detected_onsets.append(onsets)\n",
    "            ground_truth_onsets.append(time_onsetlist[i])\n",
    "    print(\"Done.\")\n",
    "    return ground_truth_onsets, detected_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute onset times using different aubio methods (Function to be used in parallel processing)\n",
    "def compute_onset(time_onset, filename, method='default'):\n",
    "    ground_truth_onsets = []\n",
    "    detected_onsets = []\n",
    "    if wave_is_pcm_encoded(os.path.join(DB_PATH,filename)):\n",
    "        onsets = detect_onsets(os.path.join(DB_PATH,filename), method=method, samplerate=SR)\n",
    "        detected_onsets.extend(onsets)\n",
    "        ground_truth_onsets.extend(time_onset)\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    return ground_truth_onsets, detected_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for method in AUBIO_ONSET_METHODS:\n",
    "    # results[method] = compute_onset_list(packedData, method=method)\n",
    "    \n",
    "    # Parallelize the computation\n",
    "    print(\"Computing onsets using %s method...\"%method)\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "  \n",
    "    result = [pool.apply_async(compute_onset, args=(time_onset, filename, method)) for time_onset, filename in zip(time_onsetlist, filenames)]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    detected_onsets = []\n",
    "    ground_truth_onsets = []\n",
    "    for r in result:\n",
    "        ground, detected = r.get()\n",
    "        if ground and detected:\n",
    "            ground_truth_onsets.append(ground)\n",
    "            detected_onsets.append(detected)\n",
    "    \n",
    "    results[method] = (ground_truth_onsets, detected_onsets)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Onset Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size for onset evaluation\n",
    "WINDOW = 0.025\n",
    "\n",
    "# Calculate the error between ground truth and detected onsets\n",
    "def onset_evaluation(groundtruth_onsets, detected_onsets, window=WINDOW):\n",
    "    # Compute the onset error using mir_eval\n",
    "    groundtruth_onsets = np.array(groundtruth_onsets)\n",
    "    detected_onsets = np.array(detected_onsets)\n",
    "    error = mir_eval.onset.evaluate(groundtruth_onsets, detected_onsets, window=window)\n",
    "    return error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the onset delay for a particular method depending of the hop_size and sample rate\n",
    "def onset_delay(method, hop_size=HOP_SIZE, sr=SR):\n",
    "    if method == \"complex\":\n",
    "        return 4.6*hop_size/sr\n",
    "    else:\n",
    "        return 4.3*hop_size/sr \n",
    "\n",
    "\n",
    "# Find the closest element in detected_onset inside the range(groundtruth_onset-window1, groundtruth_onsets+window2)\n",
    "def find_closest_element(groundtruth_onset, detected_onsets, window1, window2):\n",
    "    # Find the closest element of detected_onsets inside range [groundtruth_onsets[i]-window1, groundtruth_onsets[i]+window2]\n",
    "    # where window1 = groundtruth_onset[i] - groundtruth[i-1] and window2 = groundtruth_onset[i+1] - groundtruth[i]\n",
    "    # If there is no detected_onset inside the range, return None\n",
    "    detected_onsets = np.array(detected_onsets)\n",
    "    detected_onsets = detected_onsets[(detected_onsets >= groundtruth_onset-window1) & (detected_onsets <= groundtruth_onset+window2)]\n",
    "    if len(detected_onsets) == 0:\n",
    "        return None, None\n",
    "    closest_element = detected_onsets[np.argmin(np.abs(groundtruth_onset - detected_onsets))]\n",
    "    error = closest_element - groundtruth_onset\n",
    "    return closest_element, error\n",
    "    \n",
    "\n",
    "# Calculate the error between ground truth and detected onsets\n",
    "def onset_error(groundtruth_onsets, detected_onsets, onset_delay=0.05):\n",
    "    # Compute the onset error using mir_eval\n",
    "    groundtruth_onsets = np.array(groundtruth_onsets)\n",
    "    detected_onsets = np.array(detected_onsets)\n",
    "    \n",
    "    # For each element of groundtruth_onsets, find the closest element in detected_onsets and calculate the errors\n",
    "    errors = []\n",
    "    for i in range(len(groundtruth_onsets)):\n",
    "        # Find the closest element of detected_onsets inside range [groundtruth_onsets[i]-window1, groundtruth_onsets[i]+window2]\n",
    "        # where window1 = groundtruth_onset[i] - groundtruth[i-1] and window2 = groundtruth_onset[i+1] - groundtruth[i]\n",
    "        if i == 0:\n",
    "            window1 = 0\n",
    "        else:\n",
    "            window1 = onset_delay\n",
    "        \n",
    "        if i == len(groundtruth_onsets)-1:\n",
    "            window2 = 0\n",
    "        else:\n",
    "            window2 = 0.1*(groundtruth_onsets[i+1] - groundtruth_onsets[i])\n",
    "            \n",
    "        closest_element, error = find_closest_element(groundtruth_onsets[i], detected_onsets, window1, window2)\n",
    "        if error is not None and np.abs(error) <= 0.05:\n",
    "            errors.append(error)\n",
    "    return np.mean(np.abs(errors)), errors\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onset_error(groundtruth_onsets, detected_onsets, onset_delay=24.5):\n",
    "#     # Compute the onset error using mir_eval\n",
    "#     groundtruth_onsets = np.array(groundtruth_onsets)\n",
    "#     detected_onsets = np.array(detected_onsets)\n",
    "    \n",
    "#     # For each element of groundtruth_onsets, find the closest element in detected_onsets\n",
    "#     # and calculate the error\n",
    "#     errors = []\n",
    "#     for i, onset in enumerate(groundtruth_onsets):\n",
    "#         # For onset in groundtruth_onsets, find the closest onset in detected_onsets, which is the range groundtruth_onsets[i]- and groundtruth_onsets[i]+onset_delay\n",
    "#         closest_onset = min(detected_onsets, key=lambda x: abs(x-onset))\n",
    "#         # Calculate the error\n",
    "#         error = closest_onset-onset\n",
    "#         if i == 0:\n",
    "#             errors.append(error)\n",
    "#         else:\n",
    "#             if (np.abs(error) < 0.5*(onset - groundtruth_onsets[i-1])):\n",
    "#                 errors.append(error)\n",
    "                \n",
    "#     return np.mean(np.abs(errors)), errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each method calculate the error between ground truth and detected onsets\n",
    "method_errors = {}\n",
    "method_mae = {}\n",
    "method_metrics = {}\n",
    "for method in AUBIO_ONSET_METHODS:\n",
    "    print(\"Calculating error for method: %s\"%method)\n",
    "    print(method, \"- onset_delay: %.2f [ms]\"%(onset_delay(method)*1000))\n",
    "    method_errors[method] = []\n",
    "    method_metrics[method] = {}\n",
    "    ground_truth_onsets, detected_onsets = results[method]\n",
    "    for cur_ground, cur_detected in zip(ground_truth_onsets, detected_onsets):\n",
    "        mae, error = onset_error(cur_ground, cur_detected, onset_delay=onset_delay(method))\n",
    "        method_errors[method].append(error)\n",
    "        # Calculate evaluation metrics \n",
    "        metrics = onset_evaluation(cur_ground, cur_detected)\n",
    "        method_metrics[method].update(metrics)\n",
    "        \n",
    "    # Calculaxte the mean error for the method\n",
    "    method_mae[method] = np.mean(np.abs(np.concatenate(method_errors[method])))\n",
    "    print('+-------------------------------------+')        \n",
    "    print(\"Mean Absolute Error %.2f [ms]\"%(method_mae[method]*1000))    \n",
    "    # Calculate the mean evaluation metrics\n",
    "    for key in method_metrics[method].keys():\n",
    "        method_metrics[method][key] = np.mean(method_metrics[method][key])\n",
    "    \n",
    "    print('+-------------------------------------+')        \n",
    "    print('| Precision: %f'%metrics['Precision'])\n",
    "    \n",
    "    print('| Recall: %f'%metrics['Recall'])\n",
    "    print('| F-measure: %f'%metrics['F-measure'])\n",
    "    print('+-------------------------------------+')        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "RESFOLDER = 'results/onset_detection_PDFs'\n",
    "os.makedirs(RESFOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the probability distribution function for each method\n",
    "PLOT_TIME_IN_MS = True #Else is in seconds\n",
    "import copy\n",
    "\n",
    "def plot_error_distribution(errors_s, method):\n",
    "    # Estimate the probability density function\n",
    "    if PLOT_TIME_IN_MS:\n",
    "        errors = np.array(errors_s)*1000\n",
    "\n",
    "    # minmax_time = 50 if PLOT_TIME_IN_MS else 0.05\n",
    "    minmax_time = 10 if PLOT_TIME_IN_MS else 0.01\n",
    "\n",
    "    kde = gaussian_kde(errors)\n",
    "    x = np.linspace(-1*minmax_time, minmax_time, 10000)\n",
    "    # Compute the unnormalized PDF\n",
    "    pdf = kde(x)\n",
    "    \n",
    "    # Compute mean and variance of the data\n",
    "    mean_error = np.mean(errors)\n",
    "    variance_error = np.var(errors)\n",
    "\n",
    "    plt.clf()\n",
    "    # Plot the PDF\n",
    "    plt.plot(x, pdf, label='PDF', color='b')\n",
    "\n",
    "        # Plot mean and variance as vertical lines\n",
    "    plt.axvline(x=mean_error, color='r', linestyle='--', label='Mean')\n",
    "    plt.axvline(x=mean_error + np.sqrt(variance_error), color='g', linestyle='--', label='Mean + Variance')\n",
    "    plt.axvline(x=mean_error - np.sqrt(variance_error), color='g', linestyle='--', label='Mean - Variance')\n",
    "\n",
    "    plt.title('Probability Density Function (PDF) for %s'%method)\n",
    "    plt.xlabel('Error (%s)'%('ms' if PLOT_TIME_IN_MS else 's'))\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.xlim(-1*minmax_time, minmax_time)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #plt.savefig(os.path.join(RESFOLDER, 'PDF_%s.pdf'%method))\n",
    "    #plt.clf()\n",
    "\n",
    "    return (mean_error, variance_error, np.sqrt(variance_error), np.percentile(errors, 75) - np.percentile(errors, 25))\n",
    "\n",
    "# Mean, variance, sd, iqr all in a pd.dataframe\n",
    "statistics = pd.DataFrame(columns=['mean', 'variance', 'sd', 'iqr'])\n",
    "for method in AUBIO_ONSET_METHODS:\n",
    "    statistics_tpl = plot_error_distribution(np.concatenate(method_errors[method], axis=None), method)\n",
    "    #Add tuple as row in dataframe and add index\n",
    "    statistics.loc[method] = statistics_tpl\n",
    "    minmax_time = 50 if PLOT_TIME_IN_MS else 0.05\n",
    "    # plot_histogram(np.concatenate(method_errors[method], axis=None), method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in AUBIO_ONSET_METHODS:\n",
    "    print(\"statistics in [ms] for method: %s\"%method)\n",
    "    print('+-------------------------------------+')        \n",
    "    print(statistics.loc[method])\n",
    "    print('+-------------------------------------+')        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(errors_s, method):\n",
    "    if PLOT_TIME_IN_MS:\n",
    "        errors = np.array(errors_s)*1000\n",
    "    plt.hist(errors, bins=10000, color='b', alpha=0.7)\n",
    "    plt.title('Histogram of errors for %s'%method)\n",
    "    plt.xlabel('Error (%s)'%('ms' if PLOT_TIME_IN_MS else 's'))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(-1*minmax_time, minmax_time)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #plt.savefig(os.path.join(RESFOLDER, 'PDF_hist_%s.png'%method))\n",
    "    \n",
    "for method in AUBIO_ONSET_METHODS:\n",
    "    plot_histogram(np.concatenate(method_errors[method], axis=None), method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat_tex_table = statistics.to_latex(float_format=\"%.0f\", \n",
    "#                                     caption=\"Statistics of the onset detection error for each aubio method. Error is measured in Milliseconds\", \n",
    "#                                     bold_rows=True,\n",
    "#                                     label=\"tab:aubio_onset_statistics\")\n",
    "\n",
    "# LATEX_HEADER = r\"\"\"\n",
    "# \\documentclass{article}\n",
    "# \\usepackage{booktabs}\n",
    "# \\usepackage{graphicx} % Required for inserting images\n",
    "\n",
    "# \\title{AudioMostly24-Appendix}\n",
    "# \\date{April 2024}\n",
    "\n",
    "# \\begin{document}\n",
    "\n",
    "# \\maketitle\n",
    "# \"\"\"\n",
    "# LATEX_FOOTER = r\"\"\"\\end{document}\"\"\"\n",
    "\n",
    "# with open(os.path.join(RESFOLDER, 'appendix.tex'),\"w\") as ftexa:\n",
    "#     printf = lambda x: print(x, file=ftexa)\n",
    "#     printf(LATEX_HEADER)\n",
    "#     printf(stat_tex_table.replace(r'\\begin{table}','\\\\begin{table}\\n\\\\centering'))\n",
    "#     printf(\"\\\\section{Onset Detection Latency (Aubio)}\")\n",
    "#     printf('\\n\\\\subsection{Parameters}\\n')\n",
    "#     printf(\"The following parameters were used for the onset detection:\")\n",
    "#     printf(\"\\\\begin{itemize}\")\n",
    "#     printf(\"\\\\item Sampling Rate: %i\"%SR)\n",
    "#     printf(\"\\\\item Hop Size: %i\"%HOP_SIZE)\n",
    "#     printf(\"\\\\item Window Size: %i\"%WIN_SIZE)\n",
    "#     printf(\"\\\\end{itemize}\")\n",
    "#     printf('\\n\\\\subsection{Statistics}\\n')\n",
    "#     printf('Statistics in Table \\\\ref{tab:aubio_onset_statistics}.')\n",
    "#     printf('\\n\\\\subsection{PDF plots}\\n')\n",
    "#     for method in AUBIO_ONSET_METHODS:\n",
    "#         printf(\"\")\n",
    "#         printf(\"\\\\begin{figure}[htb]\")\n",
    "#         printf(\"    \\\\centering\")\n",
    "#         printf(\"    \\\\includegraphics[width=0.9\\\\textwidth]{PDF_%s.pdf}\"%method)\n",
    "#         printf(\"    \\\\caption{Probability Density Function for %s}\"%method)\n",
    "#         printf(\"\\\\end{figure}\")\n",
    "#         printf(\"\")\n",
    "#     printf(LATEX_FOOTER)\n",
    "\n",
    "# #os.system(\"pdflatex -output-directory=%s %s\"%(RESFOLDER, os.path.join(RESFOLDER, 'appendix.tex')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiomostly24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
