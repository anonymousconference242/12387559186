{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Pitch detection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.aGPTset.ExpressiveGuitarTechniquesDataset as agptset\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "# import torchcrepe\n",
    "import crepe\n",
    "\n",
    "import am24utils\n",
    "from am24utils import Run\n",
    "\n",
    "\n",
    "dataset = agptset.import_db()\n",
    "\n",
    "DOTEST = False\n",
    "VERBOSE = False\n",
    "DB_PATH = 'dataset/aGPTset'\n",
    "printVerbose = lambda x: print(x) if VERBOSE else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Task 1: Performing Pitch Detection with dataset onset labels and perturbations\")\n",
    "\n",
    "# Filter the db to keep only pitched notes\n",
    "def filter_by_pitched_notes(dataset):\n",
    "    pitched_files = [file for file in dataset['files_df'].index.tolist() if 'pitched' in file]        # All filtenames that have \"pitched in the index column\"\n",
    "    pitched_files = [file for file in pitched_files if 'impro' not in file]        # Remove those that contain impro\n",
    "    \n",
    "    #Take from the dataset['noteLabels_df] only the rows that have the filenames in the second value of the multiindex\n",
    "    pitched_notes_df = dataset['noteLabels_df'].loc[dataset['noteLabels_df'].index.get_level_values(1).isin(pitched_files)]\n",
    "\n",
    "    # Take only the rows where the column \"pitch_midi\" is not NaN and < 108 (key 88 piano)\n",
    "    pitched_notes_df = pitched_notes_df[pitched_notes_df['pitch_midi'].notna()]\n",
    "    pitched_notes_df = pitched_notes_df[pitched_notes_df['pitch_midi'].astype(int) < 108]\n",
    "    assert not 'natural' in pitched_notes_df.index.get_level_values(0).tolist(), \"There are natural notes in the dataset\"\n",
    "\n",
    "    filtered_filenames = pitched_notes_df.index.get_level_values(1).tolist()\n",
    "    filtered_filenames = list(sorted(set(filtered_filenames)))\n",
    "    filtered_files_df = dataset['files_df'].loc[filtered_filenames]\n",
    "\n",
    "    return pitched_notes_df,filtered_files_df\n",
    "\n",
    "\n",
    "print(\"Filtering the Dataset...\")\n",
    "filtered_notes_db,filtered_files_db = filter_by_pitched_notes(dataset)\n",
    "# print(f\"len = {len(filtered_notes_db)}, {filtered_notes_db}\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pitch detection by loading a file and running multiple pitch detectors on windows of size window_size_samples after each onset label\n",
    "def detect_pitch_from_filename(filename, onset_list, window_size_samples, detectors = ['librosa.yin','librosa.pyin','crepe'], verbose = VERBOSE):\n",
    "    printVerbose = lambda x: print(x) if verbose else None\n",
    "    # Load the file\n",
    "    y, sr = librosa.load(filename, sr=None)\n",
    "    #if 'crepe' in detectors:\n",
    "    #    y, sr = torchcrepe.load.audio(filename)\n",
    "    \n",
    "    assert sr == 48000\n",
    "\n",
    "    totres = {}\n",
    "\n",
    "    if 'librosa.yin' in detectors or 'all' in detectors:\n",
    "        res = []\n",
    "        printVerbose(\"Running librosa.yin...\")\n",
    "        for onset in onset_list:\n",
    "            window = y[onset:onset+window_size_samples]\n",
    "            try:\n",
    "                del pitch\n",
    "            except:\n",
    "                pass\n",
    "            pitch = librosa.yin(window, \n",
    "                                fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C7'), \n",
    "                                sr = sr,\n",
    "                                frame_length=window_size_samples//2,\n",
    "                                center=False)\n",
    "            printVerbose(f\"librosa.yin: onset = {onset}, pitch = {pitch}\")\n",
    "            assert len (pitch) == 5, f\"librosa.pyin was expected to return a single value, but returned {len(pitch)} values\"\n",
    "            # print warning if all 5 values are nan\n",
    "            if np.isnan(pitch).all():\n",
    "                print(f\"Warning: all 5 values of pitch are NaN for onset {onset}\")\n",
    "            rpitch = np.nanmean(pitch)\n",
    "            res.append(rpitch.item())\n",
    "            # res.append(440.0)\n",
    "        totres['librosa.yin'] = res\n",
    "\n",
    "\n",
    "    if 'librosa.pyin' in detectors or 'all' in detectors:\n",
    "        res = []\n",
    "        printVerbose(\"Running librosa.pyin...\")\n",
    "        for onset in onset_list:\n",
    "            window = y[onset:onset+window_size_samples]\n",
    "            pitch = librosa.pyin(window, \n",
    "                                fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C7'), \n",
    "                                sr = sr,\n",
    "                                frame_length=window_size_samples//2,\n",
    "                                center=False)\n",
    "            printVerbose(f\"librosa.pyin: onset = {onset}, pitch = {pitch}\")\n",
    "            assert len(pitch) == 3, f\"librosa.pyin was expected to return 3 values, but returned {len(pitch)} values\"\n",
    "            pitch = pitch[0]\n",
    "            assert len (pitch) == 5, f\"librosa.pyin was expected to return a single value, but returned {len(pitch)} values\"\n",
    "            # print warning if all 5 values are nan\n",
    "            if np.isnan(pitch).all():\n",
    "                print(f\"Warning: all 5 values of pitch are NaN for onset {onset}\")\n",
    "            rpitch = np.nanmean(pitch)\n",
    "            assert type(rpitch) == np.float64, f\"librosa.pyin returned a value that is a {type(pitch[0])} not a float\"\n",
    "            res.append(rpitch.item())\n",
    "            # res.append(440.0)\n",
    "        totres['librosa.pyin'] = res\n",
    "    \n",
    "    if 'crepe' in detectors or 'all' in detectors:\n",
    "        #import torch\n",
    "        #device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        res = []\n",
    "        printVerbose(\"Running crepe...\")\n",
    "        for onset in onset_list:\n",
    "            window = y[onset:onset+window_size_samples]\n",
    "            #window = torch.from_numpy(window).float().to(device)\n",
    "            time, pitch, confidence, activation = crepe.predict(window, sr, viterbi=True, verbose=0)\n",
    "            #pitch = torchcrepe.predict(window, sr, hop_length=int(sr/500), batch_size=64, device=device)\n",
    "            printVerbose(f\"crepe: onset = {onset}, pitch = {pitch}\")\n",
    "            #assert len(pitch) == 3, f\"crepe was expected to return 3 values, but returned {len(pitch)} values\"\n",
    "            #pitch = pitch[0]\n",
    "            #assert len (pitch) == 5, f\"crepe was expected to return a single value, but returned {len(pitch)} values\"\n",
    "            # print warning if all 5 values are nan\n",
    "            if np.isnan(pitch).all():\n",
    "                print(f\"Warning: all 5 values of pitch are NaN for onset {onset}\")\n",
    "            rpitch = np.nanmean(pitch)\n",
    "            assert type(rpitch) == np.float64, f\"crepe returned a value that is a {type(pitch[0])} not a float\"\n",
    "            res.append(rpitch.item())\n",
    "            # res.append(440.0)\n",
    "        totres['crepe'] = res\n",
    "        #totres['crepe_confidence'] = confidence[0]\n",
    "    return totres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=2048, detectors_to_use = ['crepe'], multiprocessing = True, perturbation_distribution=None, max_perturbation_samples=0):\n",
    "    y_true = {}\n",
    "    y_pred = {}\n",
    "\n",
    "    filenames = []\n",
    "    onset_lists = []\n",
    "    ground_truth_pitches = []\n",
    "    for numidx,(index, row) in enumerate(filtered_files_db.iterrows()):\n",
    "        filename = os.path.join(DB_PATH,row['full_audiofile_path'])\n",
    "        printVerbose(f\"filename = {filename}\")\n",
    "        assert os.path.exists(filename), f\"File {filename} does not exist\"\n",
    "        \n",
    "        # Take from filtered_notes_db only the rows that have the filenames in the second value of the multiindex\n",
    "        assert index in filtered_notes_db.index.get_level_values(1).tolist(), f\"File {index} not in filtered_notes_db\"\n",
    "        # Get the 'onset_label_samples' column values for the current file\n",
    "        onset_list = filtered_notes_db.loc[filtered_notes_db.index.get_level_values(1) == index].loc[:, 'onset_label_samples'].tolist()\n",
    "        onset_list = [int(onset) for onset in onset_list]\n",
    "        printVerbose(f\"onset_list ({len(onset_list)}) = {onset_list}\")\n",
    "\n",
    "        # toprint = 'Processing file %i/%i'%(numidx+1, totnumfiles)\n",
    "        # print(toprint, end='\\r')\n",
    "\n",
    "        ground_truth_pitches_cur = [float(el) for el in filtered_notes_db.loc[filtered_notes_db.index.get_level_values(1) == index].loc[:, 'pitch_midi'].tolist()]\n",
    "\n",
    "        filenames.append(filename)\n",
    "        onset_lists.append(onset_list)\n",
    "        ground_truth_pitches.append(ground_truth_pitches_cur)\n",
    "        del onset_list, ground_truth_pitches_cur\n",
    "\n",
    "    if perturbation_distribution is not None:\n",
    "        for runidx in range(len(onset_lists)):\n",
    "            onset_list = onset_lists[runidx]\n",
    "            onset_lists[runidx] = am24utils.apply_onset_perturbation(onset_list, perturbation_distribution, max_perturbation_samples, max_perturbation_samples)\n",
    "        # if perturbation_distribution == 'uniform':\n",
    "        #     for runidx in range(len(onset_lists)):\n",
    "        #         onset_list = onset_lists[runidx]\n",
    "        #         onset_lists[runidx] = [onset + np.random.randint(-max_perturbation_samples, max_perturbation_samples) for onset in onset_list]\n",
    "        # else:\n",
    "        #     raise ValueError(\"Perturbation distribution %s not valid\"%(perturbation_distribution))\n",
    "\n",
    "    if multiprocessing:\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        results = [pool.apply_async(\n",
    "            detect_pitch_from_filename, \n",
    "            args=(filename, onset_list, window_size_samples), \n",
    "            kwds={'detectors':detectors_to_use}) for filename, onset_list in zip(filenames, onset_lists)]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        detected_pitches = [r.get() for r in results]\n",
    "        # assert len(detected_pitches) == len(filenames), f\"detected_pitches has len {len(detected_pitches)} but filenames has len {len(filenames)}\"\n",
    "    else:\n",
    "        raise NotImplementedError(\"Non-multiprocessing not implemented yet\")\n",
    "        # detected_pitches = []\n",
    "        # for filename, onset_list, ground_truth_pitches_cur in zip(filenames, onset_lists, ground_truth_pitches):\n",
    "\n",
    "\n",
    "\n",
    "        #     detected_pitches = detect_pitch_from_filename(filename, onset_list, window_size_samples, verbose=False, detectors=detectors_to_use)\n",
    "\n",
    "\n",
    "    assert len(detected_pitches) == len(filenames)\n",
    "    assert len(detected_pitches) == len(ground_truth_pitches)\n",
    "\n",
    "    for runIdx in range(len(detected_pitches)):\n",
    "        cur_detected_pitches = detected_pitches[runIdx]\n",
    "        assert type(cur_detected_pitches) == dict, f\"detected_pitches is not a dict, but a {type(detected_pitches)} instead\"\n",
    "        for pikey,pival in cur_detected_pitches.items():\n",
    "            assert type(pival) == list, f\"detected_pitches[{pival}] is not a list\"\n",
    "            assert type(pikey) == str, f\"detected_pitches key is not a string\"\n",
    "            assert len(pival) > 0, f\"detected_pitches[{pival}] has len == {len(pival)}\"\n",
    "\n",
    "            assert type(cur_detected_pitches) == dict\n",
    "            for key in cur_detected_pitches.keys():\n",
    "                assert len(cur_detected_pitches[key]) == len(ground_truth_pitches[runIdx])\n",
    "                y_true[key] = []\n",
    "                y_pred[key] = []\n",
    "                for realpitch, curcur_detected_pitches in zip(ground_truth_pitches[runIdx],cur_detected_pitches[key]):\n",
    "                    detected_pitches_midi = librosa.hz_to_midi(curcur_detected_pitches)\n",
    "                    y_true[key].append(realpitch)\n",
    "                    y_pred[key].append(detected_pitches_midi)\n",
    "\n",
    "        # break\n",
    "\n",
    "\n",
    "    func_res = {'y_true':y_true, 'y_pred':y_pred, 'window_size_samples':window_size_samples, 'perturbation_distribution':perturbation_distribution}\n",
    "    if perturbation_distribution is not None:\n",
    "        func_res['max_perturbation_samples'] = max_perturbation_samples\n",
    "    return func_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_filenames = filtered_notes_db.index.get_level_values(1).tolist()\n",
    "filtered_filenames = list(sorted(set(filtered_filenames)))\n",
    "print(f\"Unique files filtered for this task = {len(filtered_filenames)}\")\n",
    "if DOTEST:\n",
    "    filtered_filenames = filtered_filenames[:5]\n",
    "    print(f\"REDUCED TO {len(filtered_filenames)} FOR TESTING PURPOSES\")\n",
    "filtered_files_db = dataset['files_df'].loc[filtered_filenames]\n",
    "\n",
    "\n",
    "\n",
    "## Perform the pitch detection\n",
    "print(\"Performing Pitch Detection...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onsetlist_filenames_ground_truth_pitches(filtered_notes_db:pd.DataFrame, filtered_files_db:pd.DataFrame):\n",
    "    filenames = []\n",
    "    onset_lists = []\n",
    "    ground_truth_pitches = []\n",
    "    for numidx,(index, row) in enumerate(filtered_files_db.iterrows()):\n",
    "        filename = os.path.join(DB_PATH,row['full_audiofile_path'])\n",
    "        printVerbose(f\"filename = {filename}\")\n",
    "        assert os.path.exists(filename), f\"File {filename} does not exist\"\n",
    "        \n",
    "        # Take from filtered_notes_db only the rows that have the filenames in the second value of the multiindex\n",
    "        assert index in filtered_notes_db.index.get_level_values(1).tolist(), f\"File {index} not in filtered_notes_db\"\n",
    "        # Get the 'onset_label_samples' column values for the current file\n",
    "        onset_list = filtered_notes_db.loc[filtered_notes_db.index.get_level_values(1) == index].loc[:, 'onset_label_samples'].tolist()\n",
    "        onset_list = [int(onset) for onset in onset_list]\n",
    "        printVerbose(f\"onset_list ({len(onset_list)}) = {onset_list}\")\n",
    "\n",
    "        # toprint = 'Processing file %i/%i'%(numidx+1, totnumfiles)\n",
    "        # print(toprint, end='\\r')\n",
    "\n",
    "        ground_truth_pitches_cur = [float(el) for el in filtered_notes_db.loc[filtered_notes_db.index.get_level_values(1) == index].loc[:, 'pitch_midi'].tolist()]\n",
    "\n",
    "        filenames.append(filename)\n",
    "        onset_lists.append(onset_list)\n",
    "        ground_truth_pitches.append(ground_truth_pitches_cur)\n",
    "        del onset_list, ground_truth_pitches_cur\n",
    "    return onset_lists, filenames, ground_truth_pitches\n",
    "\n",
    "onset_lists, filenames, ground_truth_pitches = get_onsetlist_filenames_ground_truth_pitches(filtered_notes_db, filtered_files_db)\n",
    "assert len(onset_lists) == len(filenames) == len(ground_truth_pitches), \"The lists have different lengths\"\n",
    "\n",
    "packedData = (onset_lists, filenames, ground_truth_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_taskA(runs, packedData, classifier='librosa.yin'):\n",
    "#     onsetlist_list,filenames_list,pitch_list = packedData\n",
    "#     assert len(onsetlist_list) == len(filenames_list) == len(pitch_list), f\"Different length of lists: onsetlist_list={len(onsetlist_list)}, filenames_list={len(filenames_list)}, pitch_list={len(pitch_list)}\"\n",
    "#     for ridx,run in enumerate(runs):\n",
    "#         print('Running task A for Run:%s [%i,%i]'%(run.name,ridx+1,len(runs)), end='\\r')\n",
    "#         print('+--%s--Arguments--------------+'%(run.name))\n",
    "#         print('| Window size: %i'%run.window_size_samples)\n",
    "#         print('| Onset perturbation distribution: %s'%run.onset_perturbation_distribution)\n",
    "#         print('| Onset perturbation max samples: %i'%run.onset_perturbation_max_samples)\n",
    "#         print('| Onset perturbation min samples: %i'%run.onset_perturbation_min_samples)\n",
    "#         print('+-------------------------------------+')\n",
    "        \n",
    "#         y_true, y_pred = [],[]\n",
    "#         errors_list = []\n",
    "#         for idx, file in enumerate(filenames_list):\n",
    "#             cur_onsetlist = onsetlist_list[idx]\n",
    "#             cur_y_true = pitch_list[idx]\n",
    "            \n",
    "#             # Apply onset perturbation\n",
    "#             if run.onset_perturbation_distribution is not None:\n",
    "#                 # print('Applying onset perturbation to file %s'%cur_filename)\n",
    "#                 cur_onsetlist = am24utils.apply_onset_perturbation(cur_onsetlist, run.onset_perturbation_distribution, run.onset_perturbation_max_samples, run.onset_perturbation_min_samples)\n",
    "\n",
    "#             # Perform the pitch detection\n",
    "#             cur_y_pred = detect_pitch_from_filename(file, cur_onsetlist, run.window_size_samples, detectors=[classifier], verbose=False)[classifier]\n",
    "            \n",
    "#             # Convert the detected pitches to MIDI\n",
    "#             cur_y_pred = librosa.hz_to_midi(cur_y_pred)\n",
    "            \n",
    "#             y_true_cur = np.array(cur_y_true)\n",
    "#             y_pred_cur = np.array(cur_y_pred)\n",
    "        \n",
    "#             y_pred_cur = np.nan_to_num(y_pred_cur, nan=100000) # replace Nan with inf\n",
    "\n",
    "#             # Compute absolute errors\n",
    "#             cur_errors = np.abs(y_true_cur - y_pred_cur)\n",
    "                                \n",
    "#             # Add to the list of errors\n",
    "#             errors_list.extend(cur_errors)\n",
    "#             # Append to the list of true and predicted values\n",
    "#             y_true.extend(y_true_cur)\n",
    "#             y_pred.extend(y_pred_cur)\n",
    "            \n",
    "#         # Compute the metrics\n",
    "#         y_true = np.array(y_true)\n",
    "#         y_pred = np.array(y_pred)\n",
    "#         mae = mean_absolute_error(y_true, y_pred)\n",
    "#         print('-MEAN ABSOLUTE ERROR %f-\\n'%(mae))\n",
    "        \n",
    "#         # Append run results\n",
    "#         run.results = {'mae':mae, 'errors':errors_list}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_taskA(runs, packedData,classifier='librosa.yin'):\n",
    "    onsetlist_list,filenames_list,pitch_list = packedData\n",
    "    assert len(onsetlist_list) == len(filenames_list) == len(pitch_list), f\"Different length of lists: onsetlist_list={len(onsetlist_list)}, filenames_list={len(filenames_list)}, pitch_list={len(pitch_list)}\"    \n",
    "    for ridx,run in enumerate(runs):\n",
    "            print('Running task A for Run:%s [%i,%i]'%(run.name,ridx+1,len(runs)), end='\\r')\n",
    "            print('+--%s--Arguments--------------+'%(run.name))\n",
    "            print('| Window size: %i'%run.window_size_samples)\n",
    "            print('| Onset perturbation distribution: %s'%run.onset_perturbation_distribution)\n",
    "            print('| Onset perturbation max samples: %i'%run.onset_perturbation_max_samples)\n",
    "            print('| Onset perturbation min samples: %i'%run.onset_perturbation_min_samples)\n",
    "            print('+-------------------------------------+')\n",
    "            \n",
    "            results = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=run.window_size_samples, detectors_to_use = [classifier], multiprocessing = True, perturbation_distribution=run.onset_perturbation_distribution, max_perturbation_samples=run.onset_perturbation_max_samples)\n",
    "            \n",
    "            # calculate the metrics\n",
    "            for key in results['y_true'].keys():\n",
    "                y_true = results['y_true'][key]\n",
    "                y_pred = results['y_pred'][key]\n",
    "                errors = np.abs(np.array(y_true) - np.array(y_pred))\n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                mse = mean_squared_error(y_true, y_pred)\n",
    "                print(f\"MAE for {key} = {mae}\")\n",
    "                print(f\"MSE for {key} = {mse}\")\n",
    "                run.results = { 'y_true': y_true,\n",
    "                                'y_pred': y_pred,\n",
    "                                'errors': errors, \n",
    "                                'mae':mae, \n",
    "                                'mse':mse}\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import am24utils\n",
    "from am24utils import Run\n",
    "\n",
    "to_run_NN = am24utils.get_run_list()\n",
    "\n",
    "for ridx,run in enumerate(to_run_NN):\n",
    "    print(run.name)\n",
    "\n",
    "run_taskA(to_run_NN, packedData=packedData, classifier='crepe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_run = am24utils.get_run_list()\n",
    "\n",
    "for ridx,run in enumerate(to_run):\n",
    "    print(run.name)\n",
    "\n",
    "run_taskA(to_run, packedData=packedData, classifier='librosa.yin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allruns = {}\n",
    "\n",
    "# # import am24utils\n",
    "# # from am24utils import Run\n",
    "\n",
    "# # to_run = am24utils.get_run_list()\n",
    "\n",
    "# # for ridx,run in enumerate(to_run):\n",
    "# #     print(run.name)\n",
    "\n",
    "# # run_taskA(to_run, packedData=packedData)\n",
    "\n",
    "# # # allruns['512--noP'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=512, detectors_to_use=['librosa.yin', 'crepe'])\n",
    "# allruns['1024-noP'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=1024, detectors_to_use=['librosa.yin'])\n",
    "# allruns['2048-noP'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=2048, detectors_to_use=['librosa.yin'])\n",
    "# allruns['4096-noP'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=4096, detectors_to_use=['librosa.yin'])\n",
    "\n",
    "# # # allruns['512--unip1024'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=512, detectors_to_use=['librosa.yin'], perturbation_distribution='uniform', max_perturbation_samples=1024)\n",
    "# allruns['1024-unip1024'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=1024, detectors_to_use=['librosa.yin'], perturbation_distribution='normal', max_perturbation_samples=1024)\n",
    "# allruns['2048-unip1024'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=2048, detectors_to_use=['librosa.yin'], perturbation_distribution='normal', max_perturbation_samples=1024)\n",
    "# allruns['4096-unip1024'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=4096, detectors_to_use=['librosa.yin'], perturbation_distribution='normal', max_perturbation_samples=1024)\n",
    "\n",
    "# # # allruns['512--unip2048'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=512, detectors_to_use=['librosa.yin, 'crepe''], perturbation_distribution='uniform', max_perturbation_samples=2048)\n",
    "# # allruns['1024-unip2048'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=1024, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=2048)\n",
    "# # allruns['2048-unip2048'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=2048, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=2048)\n",
    "# # allruns['4096-unip2048'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=4096, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=2048)\n",
    "\n",
    "# # # allruns['512--unip512'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=512, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=512)\n",
    "# # allruns['1024-unip0512'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=1024, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=512)\n",
    "# # allruns['2048-unip0512'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=2048, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=512)\n",
    "# # allruns['4096-unip0512'] = detect_pitch_on_dataset(filtered_files_db, filtered_notes_db, window_size_samples=4096, detectors_to_use=['librosa.yin', 'crepe'], perturbation_distribution='uniform', max_perturbation_samples=512)\n",
    "\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle, datetime\n",
    "\n",
    "# resdir_path = os.path.join('results','task-A','date_%s'%(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")))\n",
    "# os.makedirs(resdir_path)\n",
    "\n",
    "# bakfilename  = 'taskA_Yin_results.pickle'\n",
    "\n",
    "# with open(os.path.join(resdir_path,bakfilename), 'wb') as f:\n",
    "#     pickle.dump(to_run, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in to_run:\n",
    "#     run.results['error'] = run.results['errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# am24utils.plot_runs(to_run, arg_metric = 'error', arg_plottype = 'box')\n",
    "# plt.savefig(os.path.join(resdir_path,'error_yin.png'))\n",
    "# plt.savefig(os.path.join(resdir_path,'error_yin.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import am24utils\n",
    "# from am24utils import Run\n",
    "\n",
    "# to_run_crepe = am24utils.get_run_list()\n",
    "\n",
    "# for ridx,run in enumerate(to_run_crepe):\n",
    "#     print(run.name)\n",
    "\n",
    "# run_taskA(to_run_crepe, packedData=packedData, classifier='crepe')\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, datetime\n",
    "\n",
    "\n",
    "resdir_path = os.path.join('results','task-A','date_%s'%(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")))\n",
    "os.makedirs(resdir_path)\n",
    "\n",
    "# resdir_path = \"results/task-A/date_2024-05-02_16-33-13\"\n",
    "\n",
    "bakfilename  = 'taskA_CREPE_results.pickle'\n",
    "\n",
    "with open(os.path.join(resdir_path,bakfilename), 'wb') as f:\n",
    "    pickle.dump(to_run_NN, f)\n",
    "    \n",
    "    \n",
    "am24utils.plot_runs(to_run_NN, 'errors', 'box')\n",
    "plt.savefig(os.path.join(resdir_path,'errors_crepe.png'))\n",
    "plt.savefig(os.path.join(resdir_path,'errors_crepe.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakfilename  = 'taskA_YIN_results.pickle'\n",
    "\n",
    "with open(os.path.join(resdir_path,bakfilename), 'wb') as f:\n",
    "    pickle.dump(to_run, f)\n",
    "    \n",
    "    \n",
    "am24utils.plot_runs(to_run, 'errors', 'box')\n",
    "plt.savefig(os.path.join(resdir_path,'errors_yin.png'))\n",
    "plt.savefig(os.path.join(resdir_path,'errors_yin.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib qt\n",
    "# alltoplot = []\n",
    "# alltoplotlabels = []\n",
    "# for run in to_run:\n",
    "#     y_true = run.results['y_true']\n",
    "#     y_pred = run.results['y_pred']\n",
    "#     window_size_samples = run.window_size_samples\n",
    "\n",
    "#     errors = {}\n",
    "#     error_labels = []\n",
    "#     for key in y_true:\n",
    "#         y_true_cur = np.array(y_true)\n",
    "#         y_pred_cur = np.array(y_pred)\n",
    "        \n",
    "#         y_pred_cur = np.nan_to_num(y_pred_cur, nan=100000) # replace Nan with inf\n",
    "\n",
    "#         print(\"Mean Absolute Error(\",run.name,\"\",key,\") = %.1f\"% mean_absolute_error(y_true_cur, y_pred_cur))\n",
    "#         # print(\"Mean Squared Error(\",run,\") = \", mean_squared_error(y_true_cur, y_pred_cur))\n",
    "\n",
    "#         # Plot error with boxplot\n",
    "#         errors[key] = abs(y_true_cur - y_pred_cur)\n",
    "#         error_labels.append(run.name)\n",
    "#         #error_labels.append(run.name+' '+key+' '+str(window_size_samples)+' ~%.2fms'%(window_size_samples/48))\n",
    "\n",
    "#     error_lists = [errors[key] for key in errors]\n",
    "\n",
    "\n",
    "\n",
    "#     alltoplot.append(error_lists)\n",
    "#     alltoplotlabels.append(error_labels)\n",
    "\n",
    "# # # Group alltoplot and alltoplotlabels and sort by alltoplotlabels\n",
    "# # alltoplot = [x for _, x in sorted(zip(alltoplotlabels, alltoplot))]\n",
    "# # alltoplotlabels = sorted(alltoplotlabels)\n",
    "\n",
    "# plt.figure(figsize=(3*len(alltoplot), 7))\n",
    "# # Grouped boxplot since alltoplot is a list of lists\n",
    "# xpositions = np.arange(len(alltoplotlabels))\n",
    "# for idx in range(len(alltoplotlabels)):\n",
    "#     for idx_method in range(len(alltoplot[idx])):\n",
    "#         assert len(alltoplot[idx]) == len(alltoplotlabels[idx]), f\"{len(alltoplot[idx])} {len(alltoplotlabels[idx])}\"\n",
    "#         plt.boxplot(alltoplot[idx][idx_method], positions=[xpositions[idx]+idx_method*0.5],labels=[alltoplotlabels[idx][idx_method].replace(' ', '\\n')])\n",
    "#     # plt.boxplot(alltoplot[idx], positions=[xpositions[idx]], labels=[s.replace(' ', '\\n') for s in alltoplotlabels[idx]])\n",
    "        \n",
    "#         # plt.show()  \n",
    "# plt.ylabel('Error (MIDI note)')\n",
    "# plt.title('Error in MIDI note')\n",
    "# #plt.ylim(0, 2)\n",
    "\n",
    "# plt.show()  \n",
    "\n",
    "# # plt.boxplot(alltoplot,labels=alltoplotlabels)\n",
    "# # plt.ylabel('Error (MIDI note)')\n",
    "# # plt.title('Error in MIDI note (run: %s, window size = %i)'%(run, allruns[run]['window_size_samples']))\n",
    "\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in to_run:\n",
    "#     print(run.name)\n",
    "#     print(run.results['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %matplotlib qt\n",
    "# alltoplot = []\n",
    "# alltoplotlabels = []\n",
    "# for run in sorted(list(allruns.keys())):\n",
    "#     y_true = allruns[run]['y_true']\n",
    "#     y_pred = allruns[run]['y_pred']\n",
    "#     window_size_samples = allruns[run]['window_size_samples']\n",
    "\n",
    "#     errors = {}\n",
    "#     error_labels = []\n",
    "#     for key in y_true:\n",
    "#         y_true_cur = np.array(y_true[key])\n",
    "#         y_pred_cur = np.array(y_pred[key])\n",
    "        \n",
    "#         y_pred_cur = np.nan_to_num(y_pred_cur, nan=100000) # replace Nan with inf\n",
    "\n",
    "#         print(\"Mean Absolute Error(\",run,\"\",key,\") = %.1f\"% mean_absolute_error(y_true_cur, y_pred_cur))\n",
    "#         # print(\"Mean Squared Error(\",run,\") = \", mean_squared_error(y_true_cur, y_pred_cur))\n",
    "\n",
    "#         # Plot error with boxplot\n",
    "#         errors[key] = abs(y_true_cur - y_pred_cur)\n",
    "#         error_labels.append(run+' '+key+' '+str(window_size_samples)+' ~%.2fms'%(window_size_samples/48))\n",
    "\n",
    "#     error_lists = [errors[key] for key in errors]\n",
    "\n",
    "\n",
    "\n",
    "#     alltoplot.append(error_lists)\n",
    "#     alltoplotlabels.append(error_labels)\n",
    "\n",
    "# # # Group alltoplot and alltoplotlabels and sort by alltoplotlabels\n",
    "# # alltoplot = [x for _, x in sorted(zip(alltoplotlabels, alltoplot))]\n",
    "# # alltoplotlabels = sorted(alltoplotlabels)\n",
    "\n",
    "# plt.figure(figsize=(3*len(alltoplot), 7))\n",
    "# # Grouped boxplot since alltoplot is a list of lists\n",
    "# xpositions = np.arange(len(alltoplotlabels))\n",
    "# for idx in range(len(alltoplotlabels)):\n",
    "#     for idx_method in range(len(alltoplot[idx])):\n",
    "#         assert len(alltoplot[idx]) == len(alltoplotlabels[idx]), f\"{len(alltoplot[idx])} {len(alltoplotlabels[idx])}\"\n",
    "#         plt.boxplot(alltoplot[idx][idx_method], positions=[xpositions[idx]+idx_method*0.5],labels=[alltoplotlabels[idx][idx_method].replace(' ', '\\n')])\n",
    "#     # plt.boxplot(alltoplot[idx], positions=[xpositions[idx]], labels=[s.replace(' ', '\\n') for s in alltoplotlabels[idx]])\n",
    "        \n",
    "#         # plt.show()  \n",
    "# plt.ylabel('Error (MIDI note)')\n",
    "# plt.title('Error in MIDI note')\n",
    "# #plt.ylim(0, 2)\n",
    "\n",
    "# plt.show()  \n",
    "\n",
    "# # plt.boxplot(alltoplot,labels=alltoplotlabels)\n",
    "# # plt.ylabel('Error (MIDI note)')\n",
    "# # plt.title('Error in MIDI note (run: %s, window size = %i)'%(run, allruns[run]['window_size_samples']))\n",
    "\n",
    "# # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
